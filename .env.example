# Configurações Gerais da Aplicação
APP_NAME=Teste IA API

# Configurações do Ollama
# Define onde a API deve procurar o servidor do Ollama.

# Opção A: Para rodar localmente
OLLAMA_HOST=http://localhost:11434

# Opção B: Para rodar dentro do Docker
# OLLAMA_HOST=http://ollama:11434

# Modelo de IA a ser utilizado
# Recomendado: llama3.2 (leve e rápido para CPU) ou llama3.1 (mais robusto)
OLLAMA_MODEL=llama3.2